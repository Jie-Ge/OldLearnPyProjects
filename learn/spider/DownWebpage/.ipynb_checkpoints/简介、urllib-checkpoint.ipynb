{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scrapy：https://blog.csdn.net/c406495762/article/details/72858983\n",
    "- 爬虫步骤：https://www.cnblogs.com/luchun666/p/9394149.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 爬虫简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 爬虫两大特征\n",
    "    - 下载数据\n",
    "    - 能自动在网络上流窜\n",
    "\n",
    "- 步骤\n",
    "    - 下载网页\n",
    "        - 发起请求，通过HTTP库向目标站点发起请求，也就是发送一个Request，请求可以包含额外的header等信息，等待服务器响应。\n",
    "        - 获取响应内容，如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能是HTML,Json字符串，二进制数据（图片或者视频）等类型。\n",
    "    - 提取正确信息\n",
    "        - 解析内容，得到的内容可能是HTML,可以用正则表达式，页面解析库进行解析，可能是Json,可以直接转换为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。\n",
    "    - 根据一定规则自动跳到另外的网页执行上两步内容。\n",
    "    - 保存数据， 保存形式多样，可以存为文本，也可以保存到数据库，或者保存特定格式的文件。\n",
    "- 爬虫分类\n",
    "    - 通用爬虫\n",
    "    - 专用爬虫（聚焦爬虫）：它与通用搜索引擎爬虫的区别在于： 聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息\n",
    "- python包\n",
    "    - urllib, urllib3, httplib2, requests\n",
    "    - urllib, requests常用\n",
    "- Robots协议\n",
    "    - 告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取\n",
    "    - 网站（根目录下）+/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. urllib\n",
    "- https://blog.csdn.net/daycym/article/details/82750041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 包含模块   \n",
    "    - request: 它是最基本的HTTP请求模块，可以用来模拟发送请求。打开和读取urls\n",
    "    - error: 异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行重试或者其他操作以保证程序不会意外终止。\n",
    "    - parse: 一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等 【案例02】\n",
    "    - robotparser: 主要用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，这个用的比较少。\n",
    "    \n",
    "- 网页编码问题解决\n",
    "    - chardet 可以额自动检测页面文件的编码格式，但是，可能有误\n",
    "    - 要安装\n",
    "    \n",
    "- urlopen的返回对象response的方法\n",
    "    - geturl: 返回请求对象的url\n",
    "    - info: 返回反馈对象的meta（元）信息\n",
    "    - getcode：返回http code （404,500等）\n",
    "    - 【案例01】\n",
    "- request.data 的使用\n",
    "    - 访问网络的两种方法\n",
    "        - get:\n",
    "            - 利用参数给服务器传递信息\n",
    "            - 参数文dict，然后用parse编码\n",
    "            - 【案例03】\n",
    "        - post：\n",
    "            - 一般向服务器传递参数使用\n",
    "            - post是把信息自动加密处理\n",
    "            - 如果用post信息，需要用到data参数\n",
    "            - 使用post，意味着http的请求头可能需要更改：\n",
    "                - Content-Type：application/x-www.form-urlencode\n",
    "                - Content-Length: 数据长度\n",
    "                - 简而言之：一旦更改请求方法，请注意其他请求头信息相适应\n",
    "            - urllib.parse.urlencode可以将字符串自动转换成上面的格式\n",
    "            - 【案例04】\n",
    "            - 为了更多的设置请求信息，单纯的通过urlopen函数已经不太好用\n",
    "            - 需要用到request.Request 类\n",
    "            - 【案例05】\n",
    "- urllib.error\n",
    "    - URLError产生的原因：\n",
    "        - 没网\n",
    "        - 服务器链接失败\n",
    "        - 找不到指定服务器\n",
    "        - 是OSError的子类\n",
    "        - 【案例06】\n",
    "    - HTTPError: 是URLError的一个子类\n",
    "    \n",
    "    - 两者区别：\n",
    "        - HTTPError是对用的http请求的返回码错误，如果返回错误码是400以上的，则引发HTTPError\n",
    "        - URLError对应的一般是网络出现错误，包括url错误\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例01：urlopen()\n",
    "import urllib.request # 导入urllib.request模块，提供了最基本的构造HTTP请求的方法\n",
    "\n",
    "response = urllib.request.urlopen('https://www.python.org') # 以python官网为例，把这个页面爬取下来\n",
    "print(response.read().decode('utf-8'))  # read()方法得到返回的网页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd=%E4%B8%AD%E6%96%87&key=%E5%BC%A0&value=%E4%B8%89\n",
      "http://www.baidu.com/s?wd=%E4%B8%AD%E6%96%87&key=%E5%BC%A0&value=%E4%B8%89\n"
     ]
    }
   ],
   "source": [
    "# 案例02：parse模块：对url进行参数编码\n",
    "import urllib.parse\n",
    "\n",
    "url = 'http://www.baidu.com/s?'\n",
    "params = {\n",
    "    'wd': '中文',\n",
    "    'key': '张',  \n",
    "    'value': '三'\n",
    "}\n",
    "\n",
    "str_params = urllib.parse.urlencode(params)  # 字典传参拼接方式，urlencode()和quote()相似，urlencode对多个参数转义编码\n",
    "print(str_params)  # wd=%E4%B8%AD%E6%96%87&key=%25E5%25BC%25A0&value=%E4%B8%89\n",
    "search_url = url + str_params # url合并\n",
    "print(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例03：get\n",
    "payload = {'keyword': '香港', 'salecityid': '2'}\n",
    "r = requests.get(\"http://m.ctrip.com/webapp/tourvisa/visa_list\", params=payload) \n",
    "print（r.url） # 示例为http://m.ctrip.com/webapp/tourvisa/visa_list?salecityid=2&keyword=香港"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例04：post\n",
    "'''\n",
    "1. 在百度翻译中输入单词girl，发现每敲一个字母后都有请求\n",
    "2. 请求地址是：http://fanyi.baidu.com/sug\n",
    "3. 利用Network-All-Headers, 查看，发现FormData的值是 kw：girl （最下边）\n",
    "4. 检查返回的内容格式content-type，发现是json格式内容 ==》需要用到json包 \n",
    "'''\n",
    "\n",
    "from urllib import request, parse\n",
    "import json\n",
    "\n",
    "'''\n",
    "大致流程是：\n",
    "1. 利用data构造内容，然后urlopen打开\n",
    "2. 返回一个json格式的内容\n",
    "3. 结果就应该是girl的中文释义\n",
    "'''\n",
    "\n",
    "baseurl = 'https://fanyi.baidu.com/sug'\n",
    "\n",
    "# 存放用来模拟form的数据一定是dict格式\n",
    "data = {\n",
    "    # girl是翻译输入的英文内容，应该是有用户输入，此处使用硬编码\n",
    "    'kw':'girl'\n",
    "}\n",
    "\n",
    "# 需要用parse模块对data进行编码\n",
    "data = parse.urlencode(data).encode()\n",
    "\n",
    "# 有了data，url，就可以尝试发出请求了\n",
    "rsp = request.urlopen(baseurl,data=data)\n",
    "\n",
    "json_data = rsp.read().decode()\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "\n",
    "# 吧json字符串转化成字典\n",
    "json_data = json.loads(json_data)\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "\n",
    "for item in json_data['data']:\n",
    "    print(item['k'], '--', item['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例05: request.Request 类来实现案例04的内容\n",
    "'''\n",
    "1. 在百度翻译中输入单词girl，发现每敲一个字母后都有请求\n",
    "2. 请求地址是：http://fanyi.baidu.com/sug\n",
    "3. 利用Network-All-Headers, 查看，发现FormData的值是 kw：girl （最下边）\n",
    "4. 检查返回的内容格式content-type，发现是json格式内容 ==》需要用到json包 \n",
    "'''\n",
    "\n",
    "from urllib import request, parse\n",
    "import json\n",
    "\n",
    "'''\n",
    "大致流程是：\n",
    "1. 利用data构造内容，然后urlopen打开\n",
    "2. 返回一个json格式的内容\n",
    "3. 结果就应该是girl的中文释义\n",
    "'''\n",
    "\n",
    "baseurl = 'https://fanyi.baidu.com/sug'\n",
    "\n",
    "# 存放用来模拟form的数据一定是dict格式\n",
    "data = {\n",
    "    # girl是翻译输入的英文内容，应该是有用户输入，此处使用硬编码\n",
    "    'kw':'girl'\n",
    "}\n",
    "\n",
    "# 需要用parse模块对data进行编码\n",
    "data = parse.urlencode(data).encode()\n",
    "\n",
    "# ----------------- 不一样的部分 -----------------\n",
    "# 我们需要构造一个请求头，请求头部应该至少包含传入的数据的长度\n",
    "# request要求传入的请求头是一个dict格式\n",
    "headers = {\n",
    "    # 因为使用post，至少应该包含content-length 字段\n",
    "    'Content-Length': len(data)\n",
    "}\n",
    "\n",
    "# 构造一个Request实例\n",
    "req = request.Request(url=baseurl, data=data, headers=headers)\n",
    "\n",
    "\n",
    "# 因为已经构造了一个Request的请求实例，则所有的请求信息都可以封装在Request实例中\n",
    "rsp = request.urlopen(req)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "json_data = rsp.read().decode()\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "\n",
    "# 吧json字符串转化成字典\n",
    "json_data = json.loads(json_data)\n",
    "print(type(json_data))\n",
    "print(json_data)\n",
    "\n",
    "for item in json_data['data']:\n",
    "    print(item['k'], '--', item['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLError: [Errno 11001] getaddrinfo failed\n",
      "URLErroe: <urlopen error [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# 案例06：urllib.error 中 URLError 产生的原因\n",
    "\n",
    "from urllib import request, error\n",
    "\n",
    "url = 'http://www.baiduuuuuuuuu.com'\n",
    "\n",
    "try:\n",
    "    req = request.Request(url)\n",
    "    rsp = request.urlopen(req)\n",
    "    html = rsp.read().decode()\n",
    "    print(html)\n",
    "    \n",
    "except error.URLError as e:\n",
    "    print(\"URLError: {0}\".format(e.reason))\n",
    "    print(\"URLErroe: {0}\".format(e))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
