{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jiege\\PycharmProjects\\crawl（猿人学python）\\大规模异步新闻爬虫5：实现一个同步定向新闻爬虫\\ezpymysql\\__init__.py\", line 114, in execute\n",
      "    cursor.execute(query, kwparameters or parameters)\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\cursors.py\", line 148, in execute\n",
      "    result = self._query(query)\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\cursors.py\", line 310, in _query\n",
      "    conn.query(q)\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\", line 548, in query\n",
      "    self._affected_rows = self._read_query_result(unbuffered=unbuffered)\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\", line 775, in _read_query_result\n",
      "    result.read()\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\", line 1156, in read\n",
      "    first_packet = self.connection._read_packet()\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\", line 725, in _read_packet\n",
      "    packet.raise_for_error()\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\protocol.py\", line 221, in raise_for_error\n",
      "    err.raise_mysql_exception(self._data)\n",
      "  File \"D:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\err.py\", line 143, in raise_mysql_exception\n",
      "    raise errorclass(errno, errval)\n",
      "pymysql.err.OperationalError: (1050, \"Table 'crawler_hub' already exists\")\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(1050, \"Table 'crawler_hub' already exists\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b611b426698d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m ) ENGINE=MyISAM DEFAULT CHARSET=utf8\"\"\"\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql_create\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\crawl（猿人学python）\\大规模异步新闻爬虫5：实现一个同步定向新闻爬虫\\ezpymysql\\__init__.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, query, *parameters, **kwparameters)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\crawl（猿人学python）\\大规模异步新闻爬虫5：实现一个同步定向新闻爬虫\\ezpymysql\\__init__.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, query, *parameters, **kwparameters)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwparameters\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlastrowid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\cursors.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\cursors.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_executed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, sql, unbuffered)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0msql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"surrogateescape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCOMMAND\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOM_QUERY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_affected_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_query_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_affected_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36m_read_query_result\u001b[1;34m(self, unbuffered)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMySQLResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserver_status\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m             \u001b[0mfirst_packet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfirst_packet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ok_packet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[1;34m(self, packet_type)\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbuffered_active\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbuffered_active\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m             \u001b[0mpacket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    726\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpacket\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\protocol.py\u001b[0m in \u001b[0;36mraise_for_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"errno =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_mysql_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacondaaa\\envs\\qqqq\\lib\\site-packages\\pymysql\\err.py\u001b[0m in \u001b[0;36mraise_mysql_exception\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merrorclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0merrorclass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInternalError\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0merrorclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m: (1050, \"Table 'crawler_hub' already exists\")"
     ]
    }
   ],
   "source": [
    "from ezpymysql import Connection\n",
    "\n",
    "db = Connection(\n",
    "    'localhost',\n",
    "    'crawler',\n",
    "    'root',\n",
    "    'root'\n",
    ")\n",
    "\n",
    "# 若用pymysql库，需要cursor.execute(sql) 来执行sql语句\n",
    "# cursor = db.cursor()\n",
    "\n",
    "\n",
    "\n",
    "# 此表用于存储hub页面的url,hub页面就是含有大量新闻链接、不断更新的网页\n",
    "\n",
    "# AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。\n",
    "# ENGINE 设置存储引擎，CHARSET 设置编码\n",
    "\n",
    "sql_create = \"\"\"CREATE TABLE `crawler_hub` (\n",
    "              `id` int(10) unsigned NOT NULL AUTO_INCREMENT,\n",
    "              `url` varchar(64) NOT NULL,\n",
    "              `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "              PRIMARY KEY (`id`),\n",
    "              UNIQUE KEY `url` (`url`)\n",
    "            ) ENGINE=MyISAM DEFAULT CHARSET=utf8\"\"\"\n",
    "\n",
    "t1 = db.execute(sql_create)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 此表存储html内容\n",
    "# html是大量的文本内容，压缩存储会大大减少磁盘使用量。这里，我们选用lzma压缩算法\n",
    "\n",
    "sql_create2 = '''CREATE TABLE `crawler_html` (\n",
    "              `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n",
    "              `urlhash` bigint(20) unsigned NOT NULL COMMENT 'farmhash',\n",
    "              `url` varchar(512) NOT NULL,\n",
    "              `html_lzma` longblob NOT NULL,\n",
    "              `created_at` timestamp NULL DEFAULT CURRENT_TIMESTAMP,\n",
    "              PRIMARY KEY (`id`),\n",
    "              UNIQUE KEY `urlhash` (`urlhash`)\n",
    "            ) ENGINE=InnoDB DEFAULT CHARSET=utf8'''\n",
    "t2 = db.execute(sql_create2)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'farmhash'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d913419ca16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlzma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfarmhash\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'farmhash'"
     ]
    }
   ],
   "source": [
    "import urllib.parse as urlparse\n",
    "import lzma\n",
    "import pyfarmhash\n",
    "import traceback\n",
    "\n",
    "\n",
    "from ezpymysql import Connection\n",
    "from urlpool import UrlPool\n",
    "import functions as fn\n",
    "import config\n",
    "\n",
    "class NewsCrawlerSync:\n",
    "    def __init__(self, name):\n",
    "        self.db = Connection(\n",
    "            config.db_host,\n",
    "            config.db_db,\n",
    "            config.db_user,\n",
    "            config.db_password\n",
    "        )\n",
    "        self.logger = fn.init_file_logger(name + '.log')\n",
    "        self.urlpool = UrlPool(name)\n",
    "        self.hub_hosts = None\n",
    "        self.load_hubs()\n",
    "\n",
    "    def load_hubs(self,):\n",
    "        sql = 'select url from crawler_hub'\n",
    "        data = self.db.query(sql)\n",
    "        self.hub_hosts = set()\n",
    "        hubs = []\n",
    "        for d in data:\n",
    "            host = urlparse.urlparse(d['url']).netloc\n",
    "            self.hub_hosts.add(host)\n",
    "            hubs.append(d['url'])\n",
    "        self.urlpool.set_hubs(hubs, 300)\n",
    "\n",
    "    def save_to_db(self, url, html):\n",
    "        urlhash = farmhash.hash64(url)\n",
    "        sql = 'select url from crawler_html where urlhash=%s'\n",
    "        d = self.db.get(sql, urlhash)\n",
    "        if d:\n",
    "            if d['url'] != url:\n",
    "                msg = 'farmhash collision: %s <=> %s' % (url, d['url'])\n",
    "                self.logger.error(msg)\n",
    "            return True\n",
    "        if isinstance(html, str):\n",
    "            html = html.encode('utf8')\n",
    "        html_lzma = lzma.compress(html)\n",
    "        sql = ('insert into crawler_html(urlhash, url, html_lzma) '\n",
    "               'values(%s, %s, %s)')\n",
    "        good = False\n",
    "        try:\n",
    "            self.db.execute(sql, urlhash, url, html_lzma)\n",
    "            good = True\n",
    "        except Exception as e:\n",
    "            if e.args[0] == 1062:\n",
    "                # Duplicate entry\n",
    "                good = True\n",
    "                pass\n",
    "            else:\n",
    "                traceback.print_exc()\n",
    "                raise e\n",
    "        return good\n",
    "\n",
    "    def filter_good(self, urls):\n",
    "        goodlinks = []\n",
    "        for url in urls:\n",
    "            host = urlparse.urlparse(url).netloc\n",
    "            if host in self.hub_hosts:\n",
    "                goodlinks.append(url)\n",
    "        return goodlinks\n",
    "\n",
    "    def process(self, url, ishub):\n",
    "        status, html, redirected_url = fn.downloader(url)\n",
    "        self.urlpool.set_status(url, status)\n",
    "        if redirected_url != url:\n",
    "            self.urlpool.set_status(redirected_url, status)\n",
    "        # 提取hub网页中的链接, 新闻网页中也有“相关新闻”的链接，按需提取\n",
    "        if status != 200:\n",
    "            return\n",
    "        if ishub:\n",
    "            newlinks = fn.extract_links_re(redirected_url, html)\n",
    "            goodlinks = self.filter_good(newlinks)\n",
    "            print(\"%s/%s, goodlinks/newlinks\" % (len(goodlinks), len(newlinks)))\n",
    "            self.urlpool.addmany(goodlinks)\n",
    "        else:\n",
    "            self.save_to_db(redirected_url, html)\n",
    "\n",
    "    def run(self,):\n",
    "        while 1:\n",
    "            urls = self.urlpool.pop(5)\n",
    "            for url, ishub in urls.items():\n",
    "                self.process(url, ishub)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = NewsCrawlerSync('yuanrenxyue')\n",
    "    crawler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qqqq]",
   "language": "python",
   "name": "qqqq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
