{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# 用来检测一个字符串的编码\n",
    "import cchardet\n",
    "# 看捕获的异常是什么内容\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 71197 http://news.baidu.com/\n"
     ]
    }
   ],
   "source": [
    "def downloader(url, timeout=10, headers=None, debug=False, binary=False):\n",
    "    _headers = {\n",
    "        'User-Agent':('Mozilla/5.0 (compatible; MSIE 9.0; ''Windows NT 6.1; Win64; x64; Trident/5.0)')\n",
    "    }\n",
    "    redirected_url = url;\n",
    "    if headers:\n",
    "        _headers = headers\n",
    "    try:\n",
    "        r = requests.get(url, headers=_headers, timeout=timeout)\n",
    "        if binary:\n",
    "            html = r.content\n",
    "        else:\n",
    "            encoding = cchardet.detect(r.content)['encoding']\n",
    "            html = r.content.decode(encoding)\n",
    "        status = r.status_code\n",
    "        redirected_url = r.url\n",
    "    except:\n",
    "        if debug:\n",
    "            traceback.print_exc()\n",
    "        msg = 'failed download:{}'.fotmat(url)\n",
    "        print(msg)\n",
    "        if binary:\n",
    "            html = b''\n",
    "        else:\n",
    "            html = ''\n",
    "        status = 0\n",
    "    return status, html, redirected_url\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'http://news.baidu.com/'\n",
    "    s, html, lost_url_found_by_jjj = downloader(url)\n",
    "    print(s, len(html), lost_url_found_by_jjj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "h = b''\n",
    "print(h)\n",
    "print(type(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL的清洗\n",
    "- http://xinwen.eastday.com/a/n181106070849091.html?qid=news.baidu.com\n",
    "- http://news.ifeng.com/a/20181106/60146589_0.shtml?_zbs_baidu_news\n",
    "- 上面两个带?的网站来自百度新闻的首页，这个问号?的作用就是告诉目标服务器，这个网址是从百度新闻链接过来的，是百度带过来的流量。但是它们的表示方式不完全一样，一个是qid=news.baidu.com， 一个是_zbs_baidu_news。这有可能是目标服务器要求的格式不同导致的，这个在目标服务器的后台的浏览统计程序中可能用得到。\n",
    "然后去掉问号?及其后面的字符，发现它们和不去掉指向的是相同的新闻网页。\n",
    "从字符串对比上看，有问号和没问号是两个不同的网址，但是它们又指向完全相同的新闻网页，说明问号后面的参数对响应内容没有任何影响。\n",
    "正在抓取新闻的大量实践后，我们发现了这样的规律：\n",
    "新闻类网址都做了大量SEO，它们把新闻网址都静态化了，基本上都是以.html, .htm, .shtml等结尾，后面再加任何请求参数都无济于事。\n",
    "但是，还是会有些新闻网站以参数id的形式动态获取新闻网页。\n",
    "那么我们抓取新闻时，就要利用这个规律，防止重复抓取。由此，我们实现一个清洗网址的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切记，不要相信requests返回的encoding，自己判断一下更放心"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qqqq]",
   "language": "python",
   "name": "qqqq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
